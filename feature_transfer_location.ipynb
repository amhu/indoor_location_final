{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf   # http://blog.topspeedsnail.com/archives/10399\n",
    "from sklearn.preprocessing import scale  # 使用scikit-learn进行数据预处理\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:444: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "training_data = pd.read_csv(\"F:/UJIndoorLoc/trainingData.csv\",engine='python',header = 0)\n",
    "train_x = scale(np.asarray(training_data.iloc[:,0:520])) #归一化\n",
    "train_y = np.asarray(training_data[\"BUILDINGID\"].map(str) + training_data[\"FLOOR\"].map(str))  \n",
    "train_y = np.asarray(pd.get_dummies(train_y)) #进行one-hot  \n",
    "\n",
    "# X_train, X_test, Y_train, Y_test = train_test_split(train_x, train_y, test_size=0.3, random_state=0)\n",
    "# Y_train=np.asarray(pd.get_dummies(Y_train)) #进行one-hot  \n",
    "# Y_test=np.asarray(pd.get_dummies(Y_test)) #进行one-hot  \n",
    "\n",
    "# train_x=X_train\n",
    "# train_y=Y_train\n",
    "# test_x=X_test\n",
    "# test_y=Y_test\n",
    "\n",
    "test_dataset = pd.read_csv(\"F:/UJIndoorLoc/validationData.csv\",engine='python',header = 0)\n",
    "test_x = scale(np.asarray(test_dataset.iloc[:,0:520]))\n",
    "test_y = np.asarray(test_dataset[\"BUILDINGID\"].map(str) + test_dataset[\"FLOOR\"].map(str))\n",
    "test_y = np.asarray(pd.get_dummies(test_y)) #进行one-hot  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_tar, X_test_tar, Y_train_tar, Y_test_tar = train_test_split(test_x, test_y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_x=np.concatenate((train_x[:8000],test_x[:800]), axis=0)\n",
    "# train_y=np.concatenate((train_y[:8000],test_y[:800]), axis=0)\n",
    "# X_train, X_test, Y_train, Y_test = train_test_split(train_x, train_y, test_size=0.3, random_state=0)\n",
    "# test_x=test_x[801:]\n",
    "# test_y=test_y[801:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output = train_y.shape[1]\n",
    "with tf.name_scope('input'):\n",
    "    X = tf.placeholder(tf.float32, shape=[None, 520],name = 'x_input')  # 网络输入\n",
    "    Y = tf.placeholder(tf.float32,[None, output],name = 'y_input') # 网络输出\n",
    "# 定义神经网络\n",
    "batch_norm_momentum = 0.6\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "my_batch_norm_layer = partial(tf.layers.batch_normalization,training=training,momentum=batch_norm_momentum)\n",
    "# def neural_networks():\n",
    "\t# --------------------- Encoder -------------------- #\n",
    "with tf.name_scope('encoder'):\n",
    "    e_w_1 = tf.Variable(tf.truncated_normal([520, 256], stddev = 0.1),name='e_w_1')\n",
    "    e_b_1 = tf.Variable(tf.constant(0.0, shape=[256]),name='e_b_1')\n",
    "    e_w_2 = tf.Variable(tf.truncated_normal([256, 128], stddev = 0.1),name='e_w_2')\n",
    "    e_b_2 = tf.Variable(tf.constant(0.0, shape=[128]),name='e_b_2')\n",
    "    e_w_3 = tf.Variable(tf.truncated_normal([128, 64], stddev = 0.1),name='e_w_3')\n",
    "    e_b_3 = tf.Variable(tf.constant(0.0, shape=[64]),name='e_b_3')\n",
    "# --------------------- Decoder  ------------------- #\n",
    "with tf.name_scope('decoder'):\n",
    "    d_w_1 = tf.Variable(tf.truncated_normal([64, 128], stddev = 0.1),name='d_w_1')\n",
    "    d_b_1 = tf.Variable(tf.constant(0.0, shape=[128]),name='d_b_1')\n",
    "    d_w_2 = tf.Variable(tf.truncated_normal([128, 256], stddev = 0.1),name='d_w_2')\n",
    "    d_b_2 = tf.Variable(tf.constant(0.0, shape=[256]),name='d_b_2')\n",
    "    d_w_3 = tf.Variable(tf.truncated_normal([256, 520], stddev = 0.1),name='d_w_3')\n",
    "    d_b_3 = tf.Variable(tf.constant(0.0, shape=[520]),name='d_b_3')\n",
    "# --------------------- DNN  ------------------- #\n",
    "with tf.name_scope('dnn'):\n",
    "    w_1 = tf.Variable(tf.truncated_normal([64, 128], stddev = 0.1),name='w_1')\n",
    "    b_1 = tf.Variable(tf.constant(0.0, shape=[128]),name='b_1')\n",
    "    w_2 = tf.Variable(tf.truncated_normal([128, 128], stddev = 0.1),name='w_2')\n",
    "    b_2 = tf.Variable(tf.constant(0.0, shape=[128]),name='b_2')\n",
    "# --------------------- Softmax  ------------------- #\n",
    "with tf.name_scope('output'):    \n",
    "    w_3 = tf.Variable(tf.truncated_normal([128, output], stddev = 0.1),name='w_3')\n",
    "    b_3 = tf.Variable(tf.constant(0.0, shape=[output]),name='b_3')\n",
    "#########################################################\n",
    "layer_1 = tf.nn.tanh(my_batch_norm_layer(tf.add(tf.matmul(X,       e_w_1), e_b_1)))\n",
    "layer_1 = tf.layers.dropout(layer_1, keep_prob)\n",
    "\n",
    "layer_2 = tf.nn.tanh(my_batch_norm_layer(tf.add(tf.matmul(layer_1, e_w_2), e_b_2)))\n",
    "layer_2 = tf.layers.dropout(layer_2, keep_prob)\n",
    "\n",
    "encoded = tf.nn.tanh(my_batch_norm_layer(tf.add(tf.matmul(layer_2, e_w_3), e_b_3)))\n",
    "encoded = tf.layers.dropout(encoded, keep_prob)\n",
    "\n",
    "layer_4 = tf.nn.tanh(my_batch_norm_layer(tf.add(tf.matmul(encoded, d_w_1), d_b_1)))\n",
    "layer_4 = tf.layers.dropout(layer_4, keep_prob)\n",
    "\n",
    "layer_5 = tf.nn.tanh(my_batch_norm_layer(tf.add(tf.matmul(layer_4, d_w_2), d_b_2)))\n",
    "decoded = tf.nn.tanh(my_batch_norm_layer(tf.add(tf.matmul(layer_5, d_w_3), d_b_3)))\n",
    "\n",
    "layer_7 = tf.nn.tanh(my_batch_norm_layer(tf.add(tf.matmul(encoded, w_1),   b_1)))\n",
    "layer_7 = tf.layers.dropout(layer_7, keep_prob)\n",
    "layer_8 = tf.nn.tanh(my_batch_norm_layer(tf.add(tf.matmul(layer_7, w_2),   b_2)))\n",
    "out = tf.nn.softmax(tf.add(tf.matmul( layer_8, w_3),   b_3))\n",
    "# \treturn (decoded, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  Loss:  0.696978690206\n",
      "Epoch:  1  Loss:  0.669638258673\n",
      "Epoch:  2  Loss:  0.664176695099\n",
      "Epoch:  3  Loss:  0.66096118152\n",
      "Epoch:  4  Loss:  0.658741698858\n",
      "Epoch:  5  Loss:  0.657143499316\n",
      "Epoch:  6  Loss:  0.655902985316\n",
      "Epoch:  7  Loss:  0.654889446622\n",
      "Epoch:  8  Loss:  0.654100208088\n",
      "Epoch:  9  Loss:  0.653418596451\n",
      "------------------------------------------------------------------\n",
      "Epoch:  0  Loss:  5.11026153704  Accuracy:  0.929378   0.660666\n",
      "Epoch:  1  Loss:  1.40857149276  Accuracy:  0.949541   0.650765\n",
      "Epoch:  2  Loss:  0.70688988134  Accuracy:  0.974169   0.667867\n",
      "Epoch:  3  Loss:  0.36714353816  Accuracy:  0.990972   0.70297\n",
      "Epoch:  4  Loss:  0.237315686504  Accuracy:  0.995235   0.693069\n",
      "Epoch:  5  Loss:  0.202713684724  Accuracy:  0.99358   0.69667\n",
      "Epoch:  6  Loss:  0.171761010828  Accuracy:  0.996589   0.704771\n",
      "Epoch:  7  Loss:  0.160509846913  Accuracy:  0.996238   0.710171\n",
      "Epoch:  8  Loss:  0.150868761484  Accuracy:  0.995887   0.693069\n",
      "Epoch:  9  Loss:  0.146568518807  Accuracy:  0.996539   0.712871\n"
     ]
    }
   ],
   "source": [
    "# 训练神经网络\n",
    "def train_neural_networks():\n",
    "# \tdecoded, predict_output = neural_networks()\n",
    "\tpredict_output = out\n",
    "\n",
    "\tus_cost_function = tf.reduce_mean(tf.pow(X - decoded, 2))\n",
    "\ts_cost_function = -tf.reduce_sum(Y * tf.log(predict_output))\n",
    "\t#us_optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(us_cost_function)\n",
    "\tus_optimizer = tf.train.AdamOptimizer(1e-4).minimize(us_cost_function)\n",
    "\t#s_optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(s_cost_function)\n",
    "\ts_optimizer = tf.train.AdamOptimizer(1e-4).minimize(s_cost_function)\n",
    "\tcorrect_prediction = tf.equal(tf.argmax(predict_output, 1), tf.argmax(Y,1))\n",
    "\taccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "\ttraining_epochs = 10\n",
    "\tbatch_size = 20\n",
    "\ttotal_batches = training_data.shape[0]\n",
    "    \n",
    "\tsaver = tf.train.Saver()\n",
    "    \n",
    "\twith tf.Session() as sess:      \n",
    "\t\tsess.run(tf.global_variables_initializer())   # 新的\n",
    "\t\t# ------------ Traiing Autoencoders - Unsupervised Learning ----------- #\n",
    "\t\t# autoencoder是一种非监督学习算法，他利用反向传播算法，让目标值等于输入值\n",
    "\t\tfor epoch in range(training_epochs):\n",
    "\t\t\tepoch_costs = np.empty(0)\n",
    "\t\t\tfor b in range(total_batches):\n",
    "\t\t\t\toffset = (b * batch_size) % (train_x.shape[0] - batch_size)\n",
    "\t\t\t\tbatch_x = train_x[offset:(offset + batch_size), :]\n",
    "\t\t\t\t_, c = sess.run([us_optimizer, us_cost_function],feed_dict={X: batch_x,keep_prob: 0.5})\n",
    "\t\t\t\tepoch_costs = np.append(epoch_costs, c)\n",
    "\t\t\tprint(\"Epoch: \",epoch,\" Loss: \",np.mean(epoch_costs))\n",
    "\t\tprint(\"------------------------------------------------------------------\")\n",
    " \n",
    "\t\t# ---------------- Training NN - Supervised Learning ------------------ #\n",
    "# \t\tfor epoch in range(training_epochs):\n",
    "# \t\t\tepoch_costs = np.empty(0)\n",
    "# \t\t\tfor b in range(total_batches):\n",
    "# \t\t\t\toffset = (b * batch_size) % (train_x.shape[0] - batch_size)\n",
    "# \t\t\t\tbatch_x = train_x[offset:(offset + batch_size), :]\n",
    "# \t\t\t\tbatch_y = train_y[offset:(offset + batch_size), :]\n",
    "# \t\t\t\t_, c = sess.run([s_optimizer, s_cost_function],feed_dict={X: batch_x, Y : batch_y})\n",
    "# \t\t\t\tepoch_costs = np.append(epoch_costs,c)\n",
    "# \t\t\taccuracy_in_train_set = sess.run(accuracy, feed_dict={X: train_x, Y: train_y})\n",
    "# \t\t\taccuracy_in_test_set = sess.run(accuracy, feed_dict={X: test_x, Y: test_y})\n",
    "# \t\t\tprint(\"Epoch: \",epoch,\" Loss: \",np.mean(epoch_costs),\" Accuracy: \", accuracy_in_train_set, ' ', accuracy_in_test_set)\n",
    "\n",
    "\t\t# ---------------- Training NN - Supervised Learning ------------------ #\n",
    "\t\tfor epoch in range(training_epochs):\n",
    "\t\t\tepoch_costs = np.empty(0)\n",
    "\t\t\tfor b in range(total_batches):\n",
    "\t\t\t\toffset = (b * batch_size) % (train_x.shape[0] - batch_size)\n",
    "\t\t\t\tbatch_x = train_x[offset:(offset + batch_size), :]\n",
    "\t\t\t\tbatch_y = train_y[offset:(offset + batch_size), :]\n",
    "\t\t\t\t_, c = sess.run([s_optimizer, s_cost_function],feed_dict={X: batch_x, Y : batch_y,keep_prob: 0.5})\n",
    "\t\t\t\tepoch_costs = np.append(epoch_costs,c)\n",
    "\t\t\taccuracy_in_train_set = sess.run(accuracy, feed_dict={X: train_x, Y: train_y})\n",
    "\t\t\taccuracy_in_test_set = sess.run(accuracy, feed_dict={X: test_x, Y: test_y})\n",
    "\t\t\tprint(\"Epoch: \",epoch,\" Loss: \",np.mean(epoch_costs),\" Accuracy: \", accuracy_in_train_set, ' ', accuracy_in_test_set)\n",
    "\n",
    "\n",
    "\t\tsaver.save(sess, './cppp1/test-model',global_step=1000)\n",
    "        \n",
    "train_neural_networks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./cppp1\\test-model-1000\n",
      "Epoch:  0  Loss:  0.375006341356  Accuracy:  0.835264   0.766467\n",
      "Epoch:  1  Loss:  0.190820695405  Accuracy:  0.859717   0.772455\n",
      "Epoch:  2  Loss:  0.15672078365  Accuracy:  0.885457   0.772455\n",
      "Epoch:  3  Loss:  0.140091978474  Accuracy:  0.890605   0.781437\n",
      "Epoch:  4  Loss:  0.128741551366  Accuracy:  0.895753   0.784431\n",
      "Epoch:  5  Loss:  0.119989182841  Accuracy:  0.903475   0.784431\n",
      "Epoch:  6  Loss:  0.112861313659  Accuracy:  0.912484   0.784431\n",
      "Epoch:  7  Loss:  0.106869272051  Accuracy:  0.916345   0.784431\n",
      "Epoch:  8  Loss:  0.101723578302  Accuracy:  0.924067   0.778443\n",
      "Epoch:  9  Loss:  0.0972287182819  Accuracy:  0.927928   0.787425\n",
      "Epoch:  10  Loss:  0.093251643257  Accuracy:  0.929215   0.787425\n",
      "Epoch:  11  Loss:  0.0896971410499  Accuracy:  0.933076   0.793413\n",
      "Epoch:  12  Loss:  0.0864894038983  Accuracy:  0.933076   0.787425\n",
      "Epoch:  13  Loss:  0.0835806693159  Accuracy:  0.936937   0.784431\n",
      "Epoch:  14  Loss:  0.0809288025979  Accuracy:  0.939511   0.781437\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess2:\n",
    "#先加载图和参数变量\n",
    "    saver_load = tf.train.import_meta_graph('./cppp1/test-model-1000.meta')\n",
    "    saver_load.restore(sess2, tf.train.latest_checkpoint('./cppp1'))\n",
    "    graph = tf.get_default_graph()\n",
    "#     e_w_1 = graph.get_tensor_by_name(\"e_w_1:0\")\n",
    "#     e_w_2 = graph.get_tensor_by_name(\"e_w_2:0\")\n",
    "#     e_w_3 = graph.get_tensor_by_name(\"e_w_3:0\")\n",
    "#     d_w_1 = graph.get_tensor_by_name(\"d_w_1:0\")\n",
    "#     d_w_2 = graph.get_tensor_by_name(\"d_w_2:0\")\n",
    "#     d_w_3 = graph.get_tensor_by_name(\"d_w_3:0\")\n",
    "#     w_1 = graph.get_tensor_by_name(\"w1:0\")\n",
    "#     w_2 = graph.get_tensor_by_name(\"w2:0\")\n",
    "#     #w_3 = graph.get_tensor_by_name(\"w_3:0\")\n",
    "#     e_b_1 = graph.get_tensor_by_name(\"e_b_1\")\n",
    "#     e_b_2 = graph.get_tensor_by_name(\"e_b_2\")\n",
    "#     e_b_3 = graph.get_tensor_by_name(\"e_b_3\")\n",
    "#     d_b_1 = graph.get_tensor_by_name(\"d_b_1:0\")\n",
    "#     d_b_2 = graph.get_tensor_by_name(\"d_b_2:0\")\n",
    "#     d_b_3 = graph.get_tensor_by_name(\"d_b_3:0\")\n",
    "#     b_1 = graph.get_tensor_by_name(\"b_1:0\")\n",
    "#     b_2 = graph.get_tensor_by_name(\"b_2:0\")\n",
    "#     w_3 = tf.Variable(tf.truncated_normal([128, output], stddev = 0.1),name='w_3')\n",
    "#     b_3 = tf.Variable(tf.constant(0.0, shape=[output]),name='b_3')\n",
    "#     var_to_init = [w_3,b_3]\n",
    "#     tf.initialize_variables(var_to_init)\n",
    "    predict_output = out\n",
    "    correct_prediction = tf.equal(tf.argmax(predict_output, 1), tf.argmax(Y,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    training_epochs = 15\n",
    "    batch_size = 10\n",
    "    total_batches = training_data.shape[0]\n",
    "    s_cost_function = -tf.reduce_sum(Y * tf.log(predict_output)) \n",
    "    \n",
    "    g_vars=tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='output')\n",
    "    s_optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001).minimize(s_cost_function,var_list = g_vars)\n",
    "    #s_optimizer = tf.train.AdamOptimizer(1e-4).minimize(s_cost_function,var_list = g_vars)\n",
    "# ---------------- Training NN - Supervised Learning ------------------ #\n",
    "    for epoch in range(training_epochs):\n",
    "        epoch_costs = np.empty(0)\n",
    "        for b in range(total_batches):\n",
    "            offset = (b * batch_size) % (train_x.shape[0] - batch_size)\n",
    "            batch_x = X_train_tar[offset:(offset + batch_size), :]\n",
    "            batch_y = Y_train_tar[offset:(offset + batch_size), :]\n",
    "            _, c = sess2.run([s_optimizer, s_cost_function],feed_dict={X: batch_x, Y : batch_y})\n",
    "            epoch_costs = np.append(epoch_costs,c)\n",
    "#         print(\"Epoch: \",epoch,\" Loss: \",np.mean(epoch_costs))\n",
    "#     print(\"------------------------------------------------------------------\")\n",
    "\n",
    "        accuracy_in_train_set = sess2.run(accuracy, feed_dict={X: X_train_tar, Y: Y_train_tar})\n",
    "        accuracy_in_test_set = sess2.run(accuracy, feed_dict={X: X_test_tar, Y: Y_test_tar})\n",
    "        print(\"Epoch: \",epoch,\" Loss: \",np.mean(epoch_costs),\" Accuracy: \", accuracy_in_train_set, ' ', accuracy_in_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
