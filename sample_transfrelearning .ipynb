{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 样本迁移学习实现指纹定位及baseline实验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amhu/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import tree\n",
    "from sklearn.preprocessing import scale  # 使用scikit-learn进行数据预处理\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from collections import Counter\n",
    "#import xgboost as xgb\n",
    "import heapq as hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amhu/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "training_data = pd.read_csv(\"/Users/amhu/OneDrive/In文件/trainingData.csv\",engine='python',header = 0)\n",
    "train_x = scale(np.asarray(training_data.iloc[:,0:520])) #归一化\n",
    "train_y = np.asarray(training_data[\"BUILDINGID\"].map(str) + training_data[\"FLOOR\"].map(str))  \n",
    "train_y = np.asarray(pd.get_dummies(train_y)) #进行one-hot  \n",
    "\n",
    "test_dataset = pd.read_csv(\"/Users/amhu/OneDrive/In文件/validationData.csv\",engine='python',header = 0)\n",
    "test_x = scale(np.asarray(test_dataset.iloc[:,0:520]))\n",
    "test_y = np.asarray(test_dataset[\"BUILDINGID\"].map(str) + test_dataset[\"FLOOR\"].map(str))\n",
    "test_y = np.asarray(pd.get_dummies(test_y)) #进行one-hot  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_train_x,train_test_x,train_train_y,train_test_y = train_test_split(train_x,train_y,test_size = 0.3)\n",
    "train_y_index = np.argmax(train_y,1)\n",
    "test_y_index = np.argmax(test_y,1)\n",
    "train_train_y_index = np.argmax(train_train_y,1)\n",
    "train_test_y_index = np.argmax(train_test_y,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_train_x,test_test_x,test_train_y,test_test_y = train_test_split(test_x,test_y,test_size = 0.3)\n",
    "test_train_y_index = np.argmax(test_train_y,1)\n",
    "test_test_y_index = np.argmax(test_test_y,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(777, 13)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_train_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.baseline-------------------random_forest----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random_forest准确率是0.48694869486948694\n"
     ]
    }
   ],
   "source": [
    "# 标签是one-hot形式\n",
    "# test 和 train 是不同批数据\n",
    "clf_RF = RandomForestClassifier()\n",
    "clf_RF.fit(train_x,train_y)\n",
    "predicted_RF = clf_RF.predict(test_x)\n",
    "acc = accuracy_score(predicted_RF,test_y)\n",
    "print('random_forest准确率是{}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random_forest准确率是0.693969396939694\n"
     ]
    }
   ],
   "source": [
    "# 标签是类别形式\n",
    "# test 和 train 是不同批数据\n",
    "clf_RF2 = RandomForestClassifier()\n",
    "clf_RF2.fit(train_x,train_y_index)\n",
    "predicted_RF2 = clf_RF2.predict(test_x)\n",
    "acc = accuracy_score(predicted_RF2,test_y_index)\n",
    "print('random_forest准确率是{}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准确率是0.9839518555667001\n"
     ]
    }
   ],
   "source": [
    "# 标签是 类别 形式\n",
    "# test 和 train 是同批数据\n",
    "clf_RF3 = RandomForestClassifier()\n",
    "clf_RF3.fit(train_train_x,train_train_y_index)\n",
    "predicted_RF3 = clf_RF3.predict(train_test_x)\n",
    "acc = accuracy_score(predicted_RF3,train_test_y_index)\n",
    "print('准确率是{}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准确率是0.9577064526914075\n"
     ]
    }
   ],
   "source": [
    "# 标签是 one_hot 形式\n",
    "# test 和 train 是同批数据\n",
    "clf_RF4 = RandomForestClassifier()\n",
    "clf_RF4.fit(train_train_x,train_train_y)\n",
    "predicted_RF4 = clf_RF4.predict(train_test_x)\n",
    "acc = accuracy_score(predicted_RF4,train_test_y)\n",
    "print('准确率是{}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准确率为0.811377245508982,f1_score为0.811377245508982\n"
     ]
    }
   ],
   "source": [
    "# 标签是类别\n",
    "# test 和 train 都是同批test数据\n",
    "clf_RF5 = RandomForestClassifier()\n",
    "clf_RF5.fit(test_train_x,test_train_y_index)\n",
    "predicted_RF5 = clf_RF5.predict(test_test_x)\n",
    "acc = accuracy_score(predicted_RF5,test_test_y_index)\n",
    "acc_f1_score = f1_score(predicted_RF5,test_test_y_index,average='micro')\n",
    "print('准确率为{},f1_score为{}'.format(acc,acc_f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_train_x = np.concatenate((train_train_x,test_train_x),0)\n",
    "sum_train_y = np.concatenate((train_train_y_index,test_train_y_index),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准确率为0.8502994011976048,f1_score为0.8502994011976048\n"
     ]
    }
   ],
   "source": [
    "# 标签是类别\n",
    "# trian 是联合数据\n",
    "clf_RF7 = RandomForestClassifier()\n",
    "clf_RF7.fit(sum_train_x,sum_train_y)\n",
    "predicted_RF7 = clf_RF7.predict(test_test_x)\n",
    "acc = accuracy_score(predicted_RF7,test_test_y_index)\n",
    "acc_f1_score = f1_score(predicted_RF7,test_test_y_index,average='micro')\n",
    "print('准确率为{},f1_score为{}'.format(acc,acc_f1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.baseline-------------------Logistic----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic准确率是0.6885688568856886\n"
     ]
    }
   ],
   "source": [
    "# 标签是one-hot\n",
    "# test 和 train 是不同批数据\n",
    "clf_LR = LogisticRegression(multi_class='multinomial',solver = 'lbfgs')\n",
    "clf_LR.fit(train_x,train_y_index)\n",
    "predicted_LR = clf_LR.predict(test_x)\n",
    "acc = accuracy_score(predicted_LR,test_y_index)\n",
    "print('Logistic准确率是{}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准确率是0.9384821130056837\n"
     ]
    }
   ],
   "source": [
    "# 标签是类别\n",
    "# test 和 train 是同批数据\n",
    "clf_LR2 = LogisticRegression(multi_class='multinomial',solver = 'lbfgs')\n",
    "clf_LR2.fit(train_train_x,train_train_y_index)\n",
    "predicted_LR2 = clf_LR2.predict(train_test_x)\n",
    "acc = accuracy_score(predicted_LR2,train_test_y_index)\n",
    "print('准确率是{}'.format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.baseline-------------------GBDT----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBDT准确率是0.42034203420342037\n"
     ]
    }
   ],
   "source": [
    "# 标签是one-hot\n",
    "# test 和 train 是不同批数据\n",
    "clf = GradientBoostingClassifier(n_estimators=50, learning_rate=1.0,max_depth=1, random_state=0).fit(train_x,train_y_index)\n",
    "predicted_GB = clf.predict(test_x)\n",
    "acc = accuracy_score(predicted_GB,test_y_index)\n",
    "print('GBDT准确率是{}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random_forest准确率是0.7271815446339017\n"
     ]
    }
   ],
   "source": [
    "# 标签是one-hot\n",
    "# test 和 train 是不同批数据\n",
    "clf_GB2 = GradientBoostingClassifier(n_estimators=50, learning_rate=1.0,\n",
    "...     max_depth=1, random_state=0).fit(train_train_x,train_train_y_index)\n",
    "predicted_GB2 = clf.predict(train_test_x)\n",
    "acc = accuracy_score(predicted_GB2,train_test_y_index)\n",
    "print('GBDT准确率是{}'.format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.baseline-------------------KNN----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN准确率是0.6498649864986499\n"
     ]
    }
   ],
   "source": [
    "neigh = KNeighborsClassifier(n_neighbors = 3)\n",
    "neigh.fit(train_x,train_y_index)\n",
    "predicted_KNN = neigh.predict(test_x)\n",
    "acc = accuracy_score(predicted_KNN,test_y_index)\n",
    "print('KNN准确率是{}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN准确率是0.948345035105316\n"
     ]
    }
   ],
   "source": [
    "neigh2 = KNeighborsClassifier(n_neighbors = 3)\n",
    "neigh2.fit(train_train_x,train_train_y_index)\n",
    "predicted_KNN2 = neigh2.predict(train_test_x)\n",
    "acc = accuracy_score(predicted_KNN2,train_test_y_index)\n",
    "print('KNN准确率是{}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(test_y_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xg_train = xgb.DMatrix(train_x, label = train_y_index)\n",
    "xg_test  = xgb.DMatrix(test_x,label = test_y_index)\n",
    "param = {'max_depth': 2, 'eta': 0.1, 'silent': 1, 'objective': 'multi:softmax'}\n",
    "param['nthread'] = 4\n",
    "param['eval_metric'] = 'merror'\n",
    "param['num_class'] = 13\n",
    "watchlist = [ (xg_train,'train'), (xg_test, 'test') ]\n",
    "num_round = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-merror:0.312183\ttest-merror:0.49955\n",
      "[1]\ttrain-merror:0.246577\ttest-merror:0.473447\n",
      "[2]\ttrain-merror:0.223153\ttest-merror:0.457246\n",
      "[3]\ttrain-merror:0.214576\ttest-merror:0.439244\n",
      "[4]\ttrain-merror:0.192406\ttest-merror:0.423042\n",
      "[5]\ttrain-merror:0.187842\ttest-merror:0.40324\n",
      "[6]\ttrain-merror:0.181271\ttest-merror:0.391539\n",
      "[7]\ttrain-merror:0.171741\ttest-merror:0.382538\n",
      "[8]\ttrain-merror:0.165822\ttest-merror:0.372637\n",
      "[9]\ttrain-merror:0.169133\ttest-merror:0.372637\n",
      "[10]\ttrain-merror:0.155139\ttest-merror:0.359136\n",
      "[11]\ttrain-merror:0.151477\ttest-merror:0.361836\n",
      "[12]\ttrain-merror:0.146662\ttest-merror:0.348335\n",
      "[13]\ttrain-merror:0.140743\ttest-merror:0.340234\n",
      "[14]\ttrain-merror:0.13337\ttest-merror:0.342934\n",
      "[15]\ttrain-merror:0.129759\ttest-merror:0.340234\n",
      "[16]\ttrain-merror:0.127552\ttest-merror:0.327633\n",
      "[17]\ttrain-merror:0.122335\ttest-merror:0.323132\n",
      "[18]\ttrain-merror:0.115113\ttest-merror:0.313231\n",
      "[19]\ttrain-merror:0.1111\ttest-merror:0.312331\n",
      "[20]\ttrain-merror:0.108793\ttest-merror:0.311431\n",
      "[21]\ttrain-merror:0.109043\ttest-merror:0.305131\n",
      "[22]\ttrain-merror:0.105031\ttest-merror:0.305131\n",
      "[23]\ttrain-merror:0.103075\ttest-merror:0.306031\n",
      "[24]\ttrain-merror:0.102874\ttest-merror:0.306031\n"
     ]
    }
   ],
   "source": [
    "bst = xgb.train(param, xg_train, num_round, watchlist )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting, classification error=0.306031\n"
     ]
    }
   ],
   "source": [
    "pred = bst.predict( xg_test ); \n",
    "print ('predicting, classification error=%f' % (sum( int(pred[i]) != test_y_index[i] for i in range(len(test_y_index))) / float(len(test_y_index)) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.experience-------------------样本迁移----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tradaboost(trans_S, trans_A, label_S, label_A, test, test_label,N,M):\n",
    "    \n",
    "    # A是辅助域，S是源域\n",
    "    # A+S 是训练集\n",
    "    # 输入的label都是one-hot形式\n",
    "    trans_data = np.concatenate((trans_A, trans_S), axis=0)\n",
    "    trans_label = np.concatenate((label_A, label_S), axis=0)\n",
    "    all_data_AStest = np.concatenate((trans_data, test), axis=0)\n",
    "     \n",
    "#     row_A = trans_A.shape[0]\n",
    "#     row_S = trans_S.shape[0]\n",
    "#     row_T = test.shape[0]\n",
    "\n",
    "    row_A = len(trans_A)\n",
    "    row_S = len(trans_S)\n",
    "    row_T = len(test_label)\n",
    "    classifer_weights = np.zeros(N)\n",
    "    \n",
    "    # 初始化权重.weights_AS是只有辅助域和源域\n",
    "    weights_A = np.ones([row_A, 1]) / row_A\n",
    "    weights_S = np.ones([row_S, 1]) / row_S\n",
    "    weights_AS = np.concatenate((weights_A, weights_S), axis=0)\n",
    "    print('params initial finished.')\n",
    "    \n",
    "    # bata 是固定值，在源域用\n",
    "    # bata_T 是变化值，在辅助域用\n",
    "    bata = 1 / (1 + np.sqrt(2 * np.log(row_A / N)))\n",
    "    bata_T = np.zeros([1, N])\n",
    "    \n",
    "    # 输出的label 是类别.result_lable是所有的源域、辅助域、test的输出，predict是测试集输出\n",
    "    result_label_AStest = np.ones([row_A + row_S + row_T, N])\n",
    "    predict = np.zeros([row_T])\n",
    "    \n",
    "    for i in range(N): \n",
    "        P = calculate_P(weights_AS)\n",
    "        # 分类器的最终输出是 A S test 的分类结果,类别\n",
    "        print(P.reshape(len(P),1).shape)\n",
    "        \n",
    "        result_label_AStest[:, i] = train_classify(trans_data, trans_label, all_data_AStest, P)\n",
    "        print( 'result_sample', result_label_AStest[:, i][1:10], row_A, row_S, i, result_label_AStest.shape)\n",
    "        \n",
    "        #计算在源域的错误率\n",
    "        # 计算在test 的错误率\n",
    "        #error_rate = 1-f1_score(test_label,result_label_AStest[row_A + row_S:, i],average='micro')\n",
    "        error_rate = 1-accuracy_score(test_label,result_label_AStest[row_A + row_S:, i])\n",
    "        classifer_weights [i] =0.5*np.log((1-error_rate)/error_rate)\n",
    "        print('test_Error rate:', error_rate)\n",
    "        \n",
    "        if error_rate > 0.5:\n",
    "            error_rate = 0.5\n",
    "        if error_rate == 0:\n",
    "            N = i\n",
    "            break  # 防止过拟合\n",
    "            # error_rate = 0.001\n",
    "        bata_T[0, i] = error_rate / (1 - error_rate)\n",
    "        \n",
    "        #通过在辅助域的正确与否，调整辅域样本权重\n",
    "        for j in range(row_A):   \n",
    "             weights_AS[j] = weights_AS[j] * np.power(bata, compare(result_label_AStest[j, i] , label_A[j]))\n",
    "                \n",
    "        # 过在源域的正确与否，调整源域样本权重\n",
    "        for j in range(row_S):\n",
    "            weights_AS[row_A + j] = weights_AS[row_A + j] * np.power(bata_T[0, i],-compare(result_label_AStest[row_A + j, i] ,int(label_S[j])))\n",
    "    \n",
    "    # 对N个分类器进行综合，得到最终的分类结果  \n",
    "    y_predic = get_perdict(result_label_AStest[row_A + row_S:, :],classifer_weights,N)\n",
    "    acc = accuracy_score(y_predic,test_label) \n",
    "    sample = obtain_sample(weights_AS,trans_data,trans_label,M)\n",
    "    \n",
    "    return acc,weights_AS,sample\n",
    "\n",
    "\n",
    "def get_perdict(label_result,classifer_weights,N):\n",
    "    sample_num = label_result.shape[0] #样本数量\n",
    "    enc = preprocessing.OneHotEncoder(n_values=[13])  # one-hot 编码\n",
    "    sum = np.zeros((sample_num,13))  #第一个维度是样本数，第二个维度是类别数，所有分类器的加权和\n",
    "    for i in range(N): #将每个分类器的预测结果进行加权和       \n",
    "        label_1 = label_result[:,i].reshape(sample_num,1) #对应的是每个分类器的所有样本预测值，维度结构是（样本数，1）\n",
    "        label_onehot = enc.fit_transform(label_1)\n",
    "        sum = sum+ label_onehot*classifer_weights[i]\n",
    "    y_predict = np.argmax(sum,1)\n",
    "    return y_predict\n",
    "\n",
    "def train_classify(trans_data, trans_label, all_data_AStest, P):\n",
    "#     clf = tree.DecisionTreeClassifier(criterion=\"gini\", max_features=\"log2\", splitter=\"random\")\n",
    "#     clf.fit(trans_data, trans_label,P.reshape(len(P)))\n",
    "#     return clf.predict(all_data_AStest)\n",
    "    clf_LR6 = RandomForestClassifier()\n",
    "    #clf_LR6.fit(trans_data,trans_label,P.reshape(len(P)))\n",
    "    clf_LR6.fit(trans_data,trans_label)\n",
    "\n",
    "    return clf_LR6.predict(all_data_AStest)\n",
    "   \n",
    "\n",
    "def obtain_sample(weights_AS,trans_data,trans_label,M):\n",
    "    sample = []\n",
    "    index_max = weights_AS.reshape(len(weights_AS)).argsort()[-M:][::-1] \n",
    "    all = np.concatenate((trans_data,trans_label.reshape(len(trans_label),1)),1)\n",
    "    for i in index_max:\n",
    "        sample.append(all[int(i)])\n",
    "    return sample\n",
    "\n",
    "def calculate_P(weights,):\n",
    "    total = np.sum(weights)\n",
    "    return np.asarray(weights / total, order='C')        \n",
    "\n",
    "def compare(a,b):\n",
    "    if a==b:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_S = train_train_x #test_x[:350] \n",
    "label_S = train_train_y_index #np.argmax(test_y[:350],1)\n",
    " \n",
    "trans_A = test_train_x #train_x[:1000]  \n",
    "label_A = test_train_y_index #np.argmax(train_y[:1000],1)\n",
    "\n",
    "test = test_test_x #test_x[351:]\n",
    "test_label = test_test_y_index #np.argmax(test_y[351:],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params initial finished.\n",
      "(14732, 1)\n",
      "result_sample [10.  9.  2.  2.  0.  2.  1.  7.  3.] 777 13955 0 (15066, 10)\n",
      "test_Error rate: 0.14371257485029942\n",
      "(14732, 1)\n",
      "result_sample [10.  9.  2.  2.  0.  2.  1.  7.  3.] 777 13955 1 (15066, 10)\n",
      "test_Error rate: 0.16167664670658688\n",
      "(14732, 1)\n",
      "result_sample [10.  9.  2.  2.  0.  2.  1.  7.  3.] 777 13955 2 (15066, 10)\n",
      "test_Error rate: 0.16766467065868262\n",
      "(14732, 1)\n",
      "result_sample [10.  9.  2.  2.  0.  2.  1.  7.  3.] 777 13955 3 (15066, 10)\n",
      "test_Error rate: 0.16167664670658688\n",
      "(14732, 1)\n",
      "result_sample [10.  9.  2.  2.  0.  2.  1.  7.  3.] 777 13955 4 (15066, 10)\n",
      "test_Error rate: 0.13772455089820357\n",
      "(14732, 1)\n",
      "result_sample [10.  9.  2.  2.  0.  2.  1.  7.  3.] 777 13955 5 (15066, 10)\n",
      "test_Error rate: 0.1467065868263473\n",
      "(14732, 1)\n",
      "result_sample [10.  9.  2.  2.  0.  2.  1.  7.  3.] 777 13955 6 (15066, 10)\n",
      "test_Error rate: 0.1467065868263473\n",
      "(14732, 1)\n",
      "result_sample [10.  9.  2.  2.  0.  2.  1.  7.  3.] 777 13955 7 (15066, 10)\n",
      "test_Error rate: 0.16766467065868262\n",
      "(14732, 1)\n",
      "result_sample [10.  9.  2.  2.  0.  2.  1.  7.  3.] 777 13955 8 (15066, 10)\n",
      "test_Error rate: 0.15568862275449102\n",
      "(14732, 1)\n",
      "result_sample [10.  9.  2.  2.  0.  2.  1.  7.  3.] 777 13955 9 (15066, 10)\n",
      "test_Error rate: 0.1826347305389222\n"
     ]
    }
   ],
   "source": [
    "acc,weights_AS,sample = tradaboost(trans_S, trans_A, label_S, label_A, test, test_label,10,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8832335329341318"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
